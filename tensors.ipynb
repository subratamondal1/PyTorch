{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subratamondal/Workspace/PyTorch/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], dtype=torch.float16)\n",
      "1\n",
      "tensor([0., 0.], dtype=torch.float16)\n",
      "1\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]], dtype=torch.float16)\n",
      "2\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]], dtype=torch.float16)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(\n",
    "    size=(1,), # 1D Vector contains 1 elememt\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())\n",
    "\n",
    "x = torch.empty(\n",
    "    size=(2,), # 1D Vector contains 2 elememt\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())\n",
    "\n",
    "x = torch.empty(\n",
    "    size=(2,2), # 2D Matrix\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())\n",
    "\n",
    "x = torch.empty(\n",
    "    size=(2,2,3), # 3D Matrix\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9556, 0.9219],\n",
      "        [0.3491, 0.9214]], dtype=torch.float16)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(2,2),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]], dtype=torch.float16)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(\n",
    "    size=(2,2),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(\n",
    "    size=(2,2),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tensors from List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.], dtype=torch.float16)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x = [1,2,3,4,5]\n",
    "x = torch.tensor(\n",
    "    data=x,\n",
    "    dtype=torch.float16,\n",
    "    device=device,\n",
    "    requires_grad=False\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1040, 0.8472, 0.7646, 0.6636, 0.9048],\n",
      "        [0.4736, 0.1538, 0.6250, 0.5249, 0.4497],\n",
      "        [0.9390, 0.1201, 0.9341, 0.3145, 0.8247],\n",
      "        [0.2983, 0.7217, 0.2139, 0.4858, 0.6548]], dtype=torch.float16)\n",
      "2\n",
      "tensor([0.1040, 0.8472, 0.7646, 0.6636, 0.9048, 0.4736, 0.1538, 0.6250, 0.5249,\n",
      "        0.4497, 0.9390, 0.1201, 0.9341, 0.3145, 0.8247, 0.2983, 0.7217, 0.2139,\n",
      "        0.4858, 0.6548], dtype=torch.float16)\n",
      "1\n",
      "tensor([[0.1040, 0.8472, 0.7646, 0.6636],\n",
      "        [0.9048, 0.4736, 0.1538, 0.6250],\n",
      "        [0.5249, 0.4497, 0.9390, 0.1201],\n",
      "        [0.9341, 0.3145, 0.8247, 0.2983],\n",
      "        [0.7217, 0.2139, 0.4858, 0.6548]], dtype=torch.float16)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(4,5),\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(x)\n",
    "print(x.dim())\n",
    "\n",
    "y = x.view(\n",
    "    size=(20,)\n",
    ")\n",
    "print(y)\n",
    "print(y.dim())\n",
    "\n",
    "y = x.view(\n",
    "    size=(-1,4) # -1 automatically assigns value to the row based on col and total size\n",
    ")\n",
    "print(y)\n",
    "print(y.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "> Automatic Gradient (Slope) Computation\n",
    "\n",
    "- Gradients (Slopes) are essential for `Model Optimization`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3801], requires_grad=True)\n",
      "tensor([2.3801], grad_fn=<AddBackward0>)\n",
      "tensor([11.3300], grad_fn=<MulBackward0>)\n",
      "tensor([5.6650], grad_fn=<DivBackward0>)\n",
      "tensor(5.6650, grad_fn=<MeanBackward0>)\n",
      "tensor(5.6650, grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(1,),\n",
    "    requires_grad=True\n",
    ")\n",
    "print(x)\n",
    "\n",
    "y = x + 2 # creates AddBackward0\n",
    "print(y)\n",
    "\n",
    "y = y*y*2 # creates MulBackward0\n",
    "print(y)\n",
    "\n",
    "y = y/2 # creates DivBackward0\n",
    "print(y)\n",
    "\n",
    "y = y.mean() # creates MeanBackward0\n",
    "print(y)\n",
    "\n",
    "y = y.absolute() # creates AbsBackward0\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0041], requires_grad=True)\n",
      "tensor(8.0326, grad_fn=<MeanBackward0>)\n",
      "tensor([8.0163])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(1,),\n",
    "    requires_grad=True\n",
    ")\n",
    "print(x)\n",
    "\n",
    "y = x+2\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prevent Gradient Tracking\n",
    "\n",
    "```python\n",
    "# Method1\n",
    "x.requires_grad_(False)\n",
    "\n",
    "# Method2\n",
    "x.detach()\n",
    "\n",
    "# Method3\n",
    "with torch.no_grad():\n",
    "    pass\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8542, 0.3626, 0.5681], requires_grad=True)\n",
      "tensor([0.8542, 0.3626, 0.5681])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(3,),\n",
    "    requires_grad=True\n",
    ")\n",
    "print(x)\n",
    "\n",
    "# Prevent Gradient Computation\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0510, 0.2824, 0.5197], requires_grad=True)\n",
      "tensor([0.0510, 0.2824, 0.5197])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(3,),\n",
    "    requires_grad=True\n",
    ")\n",
    "print(x)\n",
    "\n",
    "# Prevent Gradient Computation\n",
    "x.detach_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6229, 0.9166, 0.5614], requires_grad=True)\n",
      "tensor([2.6229, 2.9166, 2.5614])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(\n",
    "    size=(3,),\n",
    "    requires_grad=True\n",
    ")\n",
    "print(x)\n",
    "\n",
    "# Prevent Gradient Computation\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that: Whenever we do backpropagation `.backward` that is calculate the gradients, it gets accumulated in the `.grad` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Gradient Accumulation, which results in outdated gradients >>\n",
      "tensor([1.6667, 1.6667, 1.6667])\n",
      "tensor([3.3333, 3.3333, 3.3333])\n",
      "tensor([5., 5., 5.])\n",
      "tensor([6.6667, 6.6667, 6.6667])\n",
      "tensor([8.3333, 8.3333, 8.3333])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.rand(\n",
    "    size=(3,),\n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "print(\"Example of Gradient Accumulation, which results in outdated gradients >>\")\n",
    "for epoch in range(5):\n",
    "    model_output = (weights * 5).mean()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Avoiding Gradient Accumulation to get latest gradients >>\n",
      "tensor([1.6667, 1.6667, 1.6667])\n",
      "tensor([1.6667, 1.6667, 1.6667])\n",
      "tensor([1.6667, 1.6667, 1.6667])\n",
      "tensor([1.6667, 1.6667, 1.6667])\n",
      "tensor([1.6667, 1.6667, 1.6667])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.rand(\n",
    "    size=(3,),\n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "print(\"Example of Avoiding Gradient Accumulation to get latest gradients >>\")\n",
    "for epoch in range(5):\n",
    "    model_output = (weights * 5).mean()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    \n",
    "    # Emptying the Gradients\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "**Backpropagation** is used to calculate the `gradients` of the `loss function` wrt the `weights(parameters)` of the neural network. Using the **chain rule** the Backpropagation computes the Gradients **layer by layer**, starting from the **output layer** and `moving backwards` to the **input layer**.\n",
    "\n",
    "**Computational Graph** is the graphical representation of the mathematical functions or algorithms, where the `nodes` represent `operations` and the `edges` represent the `data flow` between the operations.\n",
    "\n",
    "> Why are we interested only in the `gradient of the loss function wrt the weights(parameters)`?\n",
    "\n",
    "> Because it tells us how to adjust the `weights(parameters)` to reduce the `loss`. By following the `negative gradient`, we can find the `optimal` values of the `weights(parameters)` that `minimize` the loss. This is the `goal` of training a machine learning model.\n",
    "\n",
    "The whole concept of training a neural network:\n",
    "\n",
    "1. **Forward Pass:** Compute Loss\n",
    "2. Compute Local Gradients\n",
    "3. **Backward Pass:** Compute Gradients of the Loss wrt `weights(parameters)` (dLoss/dWeights) using the `chain rule`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "weights = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Forward Pass\n",
    "y_hat = x * weights\n",
    "# Compute Loss\n",
    "loss = (y_hat - y)**2\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "print(weights.grad)\n",
    "\n",
    "### Update Weights\n",
    "### Next Forward Pass and Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent using Autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
